{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "                                        HAYSTACK PIPELINE: SELECTION OF HOTSPOTS OF VARIABILITY AND ENRICHED MOTIFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import sys\n",
    "import subprocess as sb\n",
    "import glob\n",
    "import shutil\n",
    "import multiprocessing\n",
    "\n",
    "try:\n",
    "    import cPickle as cp\n",
    "except:\n",
    "    import pickle as cp\n",
    "    \n",
    "    \n",
    "__version__ = \"0.4.0\"\n",
    "HAYSTACK_VERSION = __version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG,\n",
    "                    format='%(levelname)-5s @ %(asctime)s:\\n\\t %(message)s \\n',\n",
    "                    datefmt='%a, %d %b %Y %H:%M:%S',\n",
    "                    stream=sys.stderr,\n",
    "                    filemode=\"w\", filename='example.log'\n",
    "                    )\n",
    "\n",
    "error = logging.critical\n",
    "warn = logging.warning\n",
    "debug = logging.debug\n",
    "info = logging.info\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logging.debug(\"start\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_library(library_name):\n",
    "    try:\n",
    "        return __import__(library_name)\n",
    "    except:\n",
    "        error('You need to install %s module to use haystack!' % library_name)\n",
    "        sys.exit(1)\n",
    "\n",
    "def which(program):\n",
    "    import os\n",
    "    def is_exe(fpath):\n",
    "        return os.path.isfile(fpath) and os.access(fpath, os.X_OK)\n",
    "\n",
    "    fpath, fname = os.path.split(program)\n",
    "    if fpath:\n",
    "        if is_exe(program):\n",
    "            return program\n",
    "    else:\n",
    "        for path in os.environ[\"PATH\"].split(os.pathsep):\n",
    "            path = path.strip('\"')\n",
    "            exe_file = os.path.join(path, program)\n",
    "            if is_exe(exe_file):\n",
    "                return exe_file\n",
    "    return None\n",
    "\n",
    "def check_program(binary_name, download_url=None):\n",
    "    if not which(binary_name):\n",
    "        error(\n",
    "            'You need to install and have the command #####%s##### in your PATH variable to use CRISPResso!\\n Please read the documentation!' % binary_name)\n",
    "        if download_url:\n",
    "            error('You can download it from here:%s' % download_url)\n",
    "        sys.exit(1)\n",
    "\n",
    "def check_file(filename):\n",
    "    try:\n",
    "        with open(filename):\n",
    "            pass\n",
    "    except IOError:\n",
    "        raise Exception('I cannot open the file: ' + filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#commmon functions haystack hotspots\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import  urllib2       \n",
    "\n",
    "def quantile_normalization(A):\n",
    "        AA = np.zeros_like(A)\n",
    "        I = np.argsort(A,axis=0)\n",
    "        AA[I,np.arange(A.shape[1])] =np.mean(A[I,np.arange(A.shape[1])],axis=1)[:,np.newaxis]\n",
    "\n",
    "        return AA\n",
    "\n",
    "def smooth(x,window_len=200):\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-1:-window_len:-1]]\n",
    "    w=np.hanning(window_len)\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y[int(window_len/2):-int(window_len/2)+1]\n",
    "\n",
    "#write the IGV session file\n",
    "def rem_base_path(path,base_path):\n",
    "            return path.replace(os.path.join(base_path,''),'')\n",
    "\n",
    "def find_th_rpm(df_chip,th_rpm):\n",
    "    return np.min(df_chip.apply(lambda x: np.percentile(x,th_rpm)))\n",
    "\n",
    "def log2_transform(x):\n",
    "    return np.log2(x+1)\n",
    "    \n",
    "def angle_transform(x):\n",
    "    return np.arcsin(np.sqrt(x)/1000000.0)\n",
    "\n",
    "\n",
    "def download_genome(name, output_directory=None):\n",
    "\n",
    "    urlpath = \"http://hgdownload.cse.ucsc.edu/goldenPath/%s/bigZips/%s.2bit\" % (name, name)\n",
    "    genome_url_origin = urllib2.urlopen(urlpath)\n",
    "\n",
    "    genome_filename=os.path.join(output_directory, \"%s.2bit\" % name)\n",
    "\n",
    "    print 'Downloading %s in %s...' %(urlpath,genome_filename)\n",
    "    \n",
    "    if os.path.exists(genome_filename):\n",
    "        print 'File %s exists, skipping download' % genome_filename\n",
    "    else:\n",
    "\n",
    "        with open(genome_filename, 'wb') as genome_file_destination:\n",
    "            shutil.copyfileobj(genome_url_origin, genome_file_destination)\n",
    "\n",
    "        print 'Downloded %s in %s:' %(urlpath,genome_filename)\n",
    "\n",
    "    g=Genome_2bit(genome_filename,verbose=True)\n",
    "\n",
    "    chr_len_filename=os.path.join(output_directory, \"%s_chr_lengths.txt\" % name)\n",
    "    if not os.path.exists(chr_len_filename):\n",
    "        print 'Extracting chromosome lengths'\n",
    "        g.write_chr_len(chr_len_filename)\n",
    "        print 'Done!'\n",
    "    else:\n",
    "        print 'File %s exists, skipping generation' % chr_len_filename\n",
    "\n",
    "    meme_bg_filename=os.path.join(output_directory, \"%s_meme_bg\" % name)\n",
    "    if not os.path.exists(meme_bg_filename):\n",
    "        print 'Calculating nucleotide frequencies....'\n",
    "        g.write_meme_background(meme_bg_filename)\n",
    "        print 'Done!'\n",
    "    else:\n",
    "        print 'File %s exists, skipping generation' % meme_bg_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#commmon functions haystack hotspots\n",
    "import numpy as np\n",
    "from scipy.stats import zscore\n",
    "import  urllib2       \n",
    "\n",
    "def quantile_normalization(A):\n",
    "        AA = np.zeros_like(A)\n",
    "        I = np.argsort(A,axis=0)\n",
    "        AA[I,np.arange(A.shape[1])] =np.mean(A[I,np.arange(A.shape[1])],axis=1)[:,np.newaxis]\n",
    "\n",
    "        return AA\n",
    "\n",
    "def smooth(x,window_len=200):\n",
    "    s=np.r_[x[window_len-1:0:-1],x,x[-1:-window_len:-1]]\n",
    "    w=np.hanning(window_len)\n",
    "    y=np.convolve(w/w.sum(),s,mode='valid')\n",
    "    return y[int(window_len/2):-int(window_len/2)+1]\n",
    "\n",
    "#write the IGV session file\n",
    "def rem_base_path(path,base_path):\n",
    "            return path.replace(os.path.join(base_path,''),'')\n",
    "\n",
    "def find_th_rpm(df_chip,th_rpm):\n",
    "    return np.min(df_chip.apply(lambda x: np.percentile(x,th_rpm)))\n",
    "\n",
    "def log2_transform(x):\n",
    "    return np.log2(x+1)\n",
    "    \n",
    "def angle_transform(x):\n",
    "    return np.arcsin(np.sqrt(x)/1000000.0)\n",
    "\n",
    "\n",
    "def download_genome(name, output_directory=None):\n",
    "\n",
    "    urlpath = \"http://hgdownload.cse.ucsc.edu/goldenPath/%s/bigZips/%s.2bit\" % (name, name)\n",
    "    genome_url_origin = urllib2.urlopen(urlpath)\n",
    "\n",
    "    genome_filename=os.path.join(output_directory, \"%s.2bit\" % name)\n",
    "\n",
    "    print 'Downloading %s in %s...' %(urlpath,genome_filename)\n",
    "    \n",
    "    if os.path.exists(genome_filename):\n",
    "        print 'File %s exists, skipping download' % genome_filename\n",
    "    else:\n",
    "\n",
    "        with open(genome_filename, 'wb') as genome_file_destination:\n",
    "            shutil.copyfileobj(genome_url_origin, genome_file_destination)\n",
    "\n",
    "        print 'Downloded %s in %s:' %(urlpath,genome_filename)\n",
    "\n",
    "    g=Genome_2bit(genome_filename,verbose=True)\n",
    "\n",
    "    chr_len_filename=os.path.join(output_directory, \"%s_chr_lengths.txt\" % name)\n",
    "    if not os.path.exists(chr_len_filename):\n",
    "        print 'Extracting chromosome lengths'\n",
    "        g.write_chr_len(chr_len_filename)\n",
    "        print 'Done!'\n",
    "    else:\n",
    "        print 'File %s exists, skipping generation' % chr_len_filename\n",
    "\n",
    "    meme_bg_filename=os.path.join(output_directory, \"%s_meme_bg\" % name)\n",
    "    if not os.path.exists(meme_bg_filename):\n",
    "        print 'Calculating nucleotide frequencies....'\n",
    "        g.write_meme_background(meme_bg_filename)\n",
    "        print 'Done!'\n",
    "    else:\n",
    "        print 'File %s exists, skipping generation' % meme_bg_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "global name 'os' is not defined",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-6df92334dc32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ROOT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0msamples_filename\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdetermine_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'haystack_analysis/sample_names.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0moutput_directory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdetermine_path\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;34m'haystack_analysis/output'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmotif_directory\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdetermine_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'motif_databases'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-6df92334dc32>\u001b[0m in \u001b[0;36mdetermine_path\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdetermine_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0m_ROOT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ROOT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msamples_filename\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdetermine_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'haystack_analysis/sample_names.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: global name 'os' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "def determine_path(folder):\n",
    "    _ROOT=os.getcwd()\n",
    "    return os.path.join(_ROOT,folder)  \n",
    "    \n",
    "samples_filename= determine_path('test_data/samples_names.txt')\n",
    "output_directory=determine_path( 'haystack_analysis/output')\n",
    "motif_directory= determine_path('motif_daatabases')\n",
    "annotation_directory=determine_path('gene_annotations')\n",
    "genome_directory=determine_path('genomes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "#mandatory\n",
    "parser = argparse.ArgumentParser(description='HAYSTACK Parameters')\n",
    "parser.add_argument('samples_filename_or_bam_folder', type=str,  help='A tab delimeted file with in each row (1) a sample name, (2) the path to the corresponding bam filename, (3 optional) the path to the corresponding gene expression filaneme. Alternatively it is possible to specify a folder containing some .bam files to analyze.')\n",
    "parser.add_argument('genome_name', type=str,  help='Genome assembly to use from UCSC (for example hg19, mm9, etc.)')\n",
    "\n",
    "#optional\n",
    "parser.add_argument('--name',  help='Define a custom output filename for the report', default='')\n",
    "parser.add_argument('--output_directory',type=str, help='Output directory (default: current directory)',default='')\n",
    "parser.add_argument('--bin_size', type=int,help='bin size to use(default: 500bp)',default=500)\n",
    "parser.add_argument('--recompute_all',help='Ignore any file previously precalculated fot the command haystack_hotstpot',action='store_true')\n",
    "parser.add_argument('--depleted', help='Look for cell type specific regions with depletion of signal instead of enrichment',action='store_true')\n",
    "parser.add_argument('--input_is_bigwig', help='Use the bigwig format instead of the bam format for the input. Note: The files must have extension .bw',action='store_true')\n",
    "parser.add_argument('--disable_quantile_normalization',help='Disable quantile normalization (default: False)',action='store_true')\n",
    "parser.add_argument('--transformation',type=str,help='Variance stabilizing transformation among: none, log2, angle (default: angle)',default='angle',choices=['angle', 'log2', 'none'])\n",
    "parser.add_argument('--z_score_high', type=float,help='z-score value to select the specific regions(default: 1.5)',default=1.5)\n",
    "parser.add_argument('--z_score_low', type=float,help='z-score value to select the not specific regions(default: 0.25)',default=0.25)\n",
    "parser.add_argument('--th_rpm',type=float,help='Percentile on the signal intensity to consider for the hotspots (default: 99)', default=99)\n",
    "parser.add_argument('--meme_motifs_filename', type=str, help='Motifs database in MEME format (default JASPAR CORE 2016)')\n",
    "parser.add_argument('--motif_mapping_filename', type=str, help='Custom motif to gene mapping file (the default is for JASPAR CORE 2016 database)')\n",
    "parser.add_argument('--plot_all',  help='Disable the filter on the TF activity and correlation (default z-score TF>0 and rho>0.3)',action='store_true')\n",
    "parser.add_argument('--n_processes',type=int, help='Specify the number of processes to use. The default is #cores available.',default=multiprocessing.cpu_count())\n",
    "parser.add_argument('--temp_directory',  help='Directory to store temporary files  (default: /tmp)', default='/tmp')\n",
    "parser.add_argument('--version',help='Print version and exit.',action='version', version='Version %s' % HAYSTACK_VERSION)\n",
    "args = parser.parse_args()\n",
    "'''\n",
    "\n",
    "# HAYSTACK Parameters\n",
    "\n",
    "samples_filename_or_bam_folder = samples_filename\n",
    "genome_name = 'hg19'\n",
    "\n",
    "# optional\n",
    "name = ''\n",
    "output_directory = output_directory\n",
    "bin_size = 200\n",
    "recompute_all = True\n",
    "depleted = True\n",
    "input_is_bigwig = True\n",
    "disable_quantile_normalization = False\n",
    "transformation = 'angle'\n",
    "z_score_high = 1.5\n",
    "z_score_low = 0.25\n",
    "th_rpm = 99\n",
    "meme_motifs_filename = '' #os.path.join(MOTIF_DIR, 'JASPAR_CORE_2016_vertebrates.meme')\n",
    "motif_mapping_filename = '' #os.path.join(MOTIF_DIR, 'JASPAR_CORE_2016_vertebrates_mapped_to_gene_human_mouse.txt')\n",
    "plot_all = True\n",
    "use_X_Y= True\n",
    "chrom_exclude = '_|chrM|chrX|chrY'\n",
    "n_processes = multiprocessing.cpu_count()\n",
    "temp_directory = '' #os.path.join(_ROOT, 'tmp')\n",
    "version = 'Version %s' % HAYSTACK_VERSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if meme_motifs_filename:\n",
    "    check_file(meme_motifs_filename)\n",
    "\n",
    "if motif_mapping_filename:\n",
    "    check_file(motif_mapping_filename)\n",
    "    \n",
    "#if not os.path.exists(temp_directory):\n",
    "#    error('The folder specified with --temp_directory: %s does not exist!' % temp_directory)\n",
    "#    sys.exit(1)\n",
    "\n",
    "if input_is_bigwig:\n",
    "        extension_to_check='.bw'\n",
    "        info('Input is set BigWig (.bw)')\n",
    "else:\n",
    "        extension_to_check='.bam'\n",
    "        info('Input is set compressed SAM (.bam)')\n",
    "\n",
    "if name:\n",
    "        directory_name='HAYSTACK_PIPELINE_RESULTS_on_%s' % name\n",
    "\n",
    "else:\n",
    "        directory_name='HAYSTACK_PIPELINE_RESULTS'\n",
    "\n",
    "if output_directory:\n",
    "        output_directory=os.path.join(output_directory,directory_name)\n",
    "else:\n",
    "        output_directory=directory_name\n",
    "\n",
    "#check folder or sample filename\n",
    "\n",
    "USE_GENE_EXPRESSION=True\n",
    "\n",
    "if os.path.isfile(samples_filename_or_bam_folder):\n",
    "        BAM_FOLDER=False\n",
    "        bam_filenames=[]\n",
    "        gene_expression_filenames=[]\n",
    "        sample_names=[]\n",
    "        \n",
    "        dir_path = os.path.dirname(os.path.realpath(samples_filename_or_bam_folder))\n",
    "\n",
    "        with open(samples_filename_or_bam_folder) as infile:\n",
    "            for line in infile:\n",
    "\n",
    "                if not line.strip():\n",
    "                        continue\n",
    "                \n",
    "                if line.startswith('#'): #skip optional header line or empty lines\n",
    "                        info('Skipping header/comment line:%s' % line)\n",
    "                        continue\n",
    "\n",
    "                fields=line.strip().split(\"\\t\")\n",
    "                n_fields=len(fields)\n",
    "\n",
    "                if n_fields==2:\n",
    "\n",
    "                    USE_GENE_EXPRESSION=False\n",
    "                    \n",
    "                    sample_names.append(fields[0])\n",
    "                    bam_filenames.append(fields[1])\n",
    "\n",
    "                elif n_fields==3:\n",
    "\n",
    "                    USE_GENE_EXPRESSION=USE_GENE_EXPRESSION and True\n",
    "\n",
    "                    sample_names.append(fields[0])\n",
    "                    bam_filenames.append(fields[1])\n",
    "                    gene_expression_filenames.append(fields[2])\n",
    "                else:\n",
    "                    error('The samples file format is wrong!')\n",
    "                    \n",
    "        bam_filenames= [ os.path.join(dir_path,filename) for filename in bam_filenames]           \n",
    "        gene_expression_filenames= [ os.path.join(dir_path,filename) for filename in gene_expression_filenames]\n",
    "        \n",
    "else:\n",
    "        if os.path.exists(samples_filename_or_bam_folder):\n",
    "                BAM_FOLDER=True\n",
    "                USE_GENE_EXPRESSION=False\n",
    "                bam_filenames=glob.glob(os.path.join(samples_filename_or_bam_folder,'*'+extension_to_check))\n",
    "\n",
    "                if not bam_filenames:\n",
    "                    error('No bam/bigwig  files to analyze in %s. Exiting.' % samples_filename_or_bam_folder)\n",
    "                    sys.exit(1)\n",
    "                \n",
    "                sample_names=[os.path.basename(bam_filename).replace(extension_to_check,'') for bam_filename in bam_filenames]\n",
    "        else:\n",
    "                error(\"The file or folder %s doesn't exist. Exiting.\" % samples_filename_or_bam_folder)\n",
    "                sys.exit(1)\n",
    "\n",
    "#check all the files before starting\n",
    "info('Checking samples files location...')\n",
    "for bam_filename in bam_filenames:\n",
    "        check_file(bam_filename)\n",
    "\n",
    "if USE_GENE_EXPRESSION:\n",
    "    for gene_expression_filename in gene_expression_filenames:\n",
    "            check_file(gene_expression_filename)\n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "        os.makedirs(output_directory)\n",
    "\n",
    "#copy back the file used\n",
    "if not BAM_FOLDER:\n",
    "        shutil.copy2(samples_filename_or_bam_folder,output_directory)\n",
    "\n",
    "#write hotspots conf files\n",
    "sample_names_hotspots_filename=os.path.join(output_directory,'sample_names_hotspots.txt')\n",
    "\n",
    "with open(sample_names_hotspots_filename,'w+') as outfile:\n",
    "    for sample_name,bam_filename in zip(sample_names,bam_filenames):\n",
    "        outfile.write('%s\\t%s\\n' % (sample_name, bam_filename))\n",
    "\n",
    "#write tf activity  conf files\n",
    "if USE_GENE_EXPRESSION:\n",
    "        sample_names_tf_activity_filename=os.path.join(output_directory,'sample_names_tf_activity.txt')\n",
    "\n",
    "        with open(sample_names_tf_activity_filename,'w+') as outfile:\n",
    "                for sample_name,gene_expression_filename in zip(sample_names,gene_expression_filenames):\n",
    "                        outfile.write('%s\\t%s\\n' % (sample_name, gene_expression_filename))\n",
    "\n",
    "        tf_activity_directory=os.path.join(output_directory,'HAYSTACK_TFs_ACTIVITY_PLANES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "mpl.use('Agg')\n",
    "import pylab as pl\n",
    "import xml.etree.cElementTree as ET\n",
    "from pybedtools import BedTool              \n",
    "from bioutilities import Genome_2bit\n",
    "import pysam    \n",
    "import re\n",
    "info('Initializing Genome:%s' %genome_name)\n",
    "\n",
    "genome_2bit=os.path.join(genome_directory,genome_name+'.2bit')\n",
    "\n",
    "if os.path.exists(genome_2bit):\n",
    "        genome=Genome_2bit(genome_2bit)\n",
    "else:\n",
    "        info(\"\\nIt seems you don't have the required genome file.\")\n",
    "\n",
    "        download_genome(genome_name, genome_directory)\n",
    "        if os.path.exists(genome_2bit):\n",
    "                info('Genome correctly downloaded!')\n",
    "                genome=Genome_2bit(genome_2bit)\n",
    "        else:\n",
    "                error('Sorry I cannot download the required file for you. Check your Internet connection.')\n",
    "                sys.exit(1)\n",
    "\n",
    "chr_len_filename=os.path.join(genome_directory, \"%s_chr_lengths.txt\" % genome_name)\n",
    "check_file(chr_len_filename)\n",
    "\n",
    "if name:\n",
    "        directory_name='HAYSTACK_HOTSPOTS_on_%s' % name\n",
    "\n",
    "else:\n",
    "        directory_name='HAYSTACK_HOTSPOTS'\n",
    "\n",
    "\n",
    "if output_directory:\n",
    "        output_directory=os.path.join(output_directory,directory_name)\n",
    "else:\n",
    "        output_directory=directory_name\n",
    "        \n",
    "\n",
    "if not os.path.exists(output_directory):\n",
    "    os.makedirs(output_directory)    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tracks_directory=os.path.join(output_directory,'TRACKS')\n",
    "if not os.path.exists(tracks_directory):\n",
    "        os.makedirs(tracks_directory)   \n",
    "\n",
    "intermediate_directory=os.path.join(output_directory,'INTERMEDIATE')\n",
    "if not os.path.exists(intermediate_directory):\n",
    "        os.makedirs(intermediate_directory) \n",
    "        \n",
    "        \n",
    "# filter chromosomes\n",
    "\n",
    "chr_len_filtered_filename = os.path.join(genome_directory, \"%s_chr_lengths_filtered.txt\" % genome_name)      \n",
    "with open(chr_len_filtered_filename,'wb') as fout:\n",
    "    fout.writelines(line for line in open(chr_len_filename) if not re.search(chrom_exclude, line.split()[0]))\n",
    "                                    \n",
    "                                                    \n",
    "genome_sorted_bins_file=os.path.join(output_directory,'%s.%dbp.bins.sorted.bed' %(os.path.basename(genome_name),bin_size))\n",
    "genome_sorted_filtered_bins_file=os.path.join(output_directory,'%s.%dbp.bins.sorted.filterd.bed' %(os.path.basename(genome_name),bin_size))\n",
    "blacklist =os.path.join(genome_directory,'blacklist.bed')        \n",
    "\n",
    "if not os.path.exists(genome_sorted_bins_file) or recompute_all:\n",
    "        info('Creating bins of %dbp in %s' %(bin_size,genome_sorted_bins_file))\n",
    "        \n",
    "        \n",
    "        genome_windows=BedTool().window_maker(g=chr_len_filtered_filename, w=bin_size).sort().saveas(genome_sorted_bins_file)\n",
    "        \n",
    "        genome_windows_filtered = genome_windows.intersect(blacklist,\n",
    "                                                   wa=True,\n",
    "                                                   v=True,\n",
    "                                                   output= genome_sorted_filtered_bins_file)\n",
    "\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "bam_temp_filenames= [ os.path.join(dir_path,'%s.temp%s' %(os.path.splitext(os.path.basename(bam_filename)))) for bam_filename in bam_filenames]           \n",
    "bam_filtered_nodup_filenames= [ os.path.join(dir_path,'%s.filterd.nodup%s' %(os.path.splitext(os.path.basename(bam_filename)))) for bam_filename in bam_filenames]           \n",
    "\n",
    "#convert bam files to genome-wide rpm tracks\n",
    "for base_name,bam_filename in zip(sample_names,bam_filenames):\n",
    "    \n",
    "    # bam_filename=bam_filenames[0]\n",
    "    # base_name=sample_names[0]\n",
    "    # bam_temp_filename=bam_temp_filenames[0]\n",
    "    # bam_filtered_nodup_filename=bam_filtered_nodup_filenames[0]\n",
    "\n",
    "    info('Processing:%s' %bam_filename)\n",
    "    \n",
    "    rpm_filename=os.path.join(tracks_directory,'%s.bedgraph' % base_name)\n",
    "    sorted_rpm_filename=os.path.join(tracks_directory,'%s_sorted.bedgraph' % base_name)\n",
    "    mapped_sorted_rpm_filename=os.path.join(tracks_directory,'%s_mapped_sorted.bedgraph' % base_name)\n",
    "    binned_rpm_filename=os.path.join(intermediate_directory,'%s.%dbp.rpm' % (base_name,bin_size))\n",
    "    bigwig_filename=os.path.join(tracks_directory,'%s.bw' %base_name)\n",
    "\n",
    "    if  input_is_bigwig and which('bigWigAverageOverBed'):\n",
    "                    if not os.path.exists(binned_rpm_filename) or recompute_all:\n",
    "                            cmd='bigWigAverageOverBed %s %s  /dev/stdout | sort -s -n -k 1,1 | cut -f5 > %s' % (bam_filename,genome_sorted_filtered_bins_file,binned_rpm_filename)\n",
    "                            sb.call(cmd,shell=True)\n",
    "                            shutil.copy2(bam_filename,bigwig_filename)\n",
    "\n",
    "    else:    \n",
    "            if not os.path.exists(binned_rpm_filename) or recompute_all:\n",
    "                    \n",
    "                    # info('Computing Scaling Factor...')\n",
    "                    # infile = pysam.AlignmentFile(bam_filename, \"rb\")\n",
    "                    # numreads= infile.count(until_eof=True)\n",
    "                    # scaling_factor=(1.0/float(numreads))*1000000\n",
    "                    \n",
    "                    # =============================\n",
    "\t\t\t        # Remove  unmapped, mate unmapped\n",
    "\t\t\t        # not primary alignment, reads failing platform, low MAPQ reads, optical duplicate\n",
    "                    # ==================\n",
    "                    \n",
    "                    cmd= 'sambamba view -f bam -l 0 -t %d -F \"not (unmapped or mate_is_unmapped or failed_quality_control or duplicate or secondary_alignment) and mapping_quality >= 30\" \"%s\"  -o \"%s\"' %(n_processes,bam_filename, bam_temp_filename) \n",
    "                    proc=sb.call(cmd,shell=True)\n",
    "        \n",
    "                    cmd= 'sambamba markdup  -l 8 -t %d --hash-table-size=17592186044416 --overflow-list-size=20000000 --io-buffer-size=256 \"%s\" \"%s\" ' %(n_processes, bam_temp_filename, bam_filtered_nodup_filename)\n",
    "                    proc=sb.call(cmd,shell=True)\n",
    "            \n",
    "                    try:    \n",
    "                        os.remove(bam_temp_filename)\n",
    "                        os.remove(bam_temp_filename+'.bai')\n",
    "                    except:\n",
    "                        pass\n",
    "                    \n",
    "                    info('Building BedGraph RPM track...')    \n",
    "                    cmd='bamCoverage --bam \"%s\" -o \"%s\" --blackListFileName \"%s\" --binSize %d --normalizeTo1x 2150570000  --extendReads %d  --outFileFormat bedgraph  --ignoreForNormalization chrX' %(bam_filtered_nodup_filename, rpm_filename, blacklist, bin_size, bin_size)  \n",
    "                    proc=sb.call(cmd,shell=True)\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "\n",
    "            if which('bedGraphToBigWig'):\n",
    "                if not os.path.exists(bigwig_filename) or recompute_all:\n",
    "                        info('Converting BedGraph to BigWig')\n",
    "                        cmd='bedGraphToBigWig %s %s %s' %(rpm_filename,chr_len_filename,bigwig_filename)\n",
    "                        proc=sb.call(cmd,shell=True)\n",
    "\n",
    "            else:\n",
    "                info('Sorry I cannot create the bigwig file.\\nPlease download and install bedGraphToBigWig from here: http://hgdownload.cse.ucsc.edu/admin/exe/ and add to your PATH')\n",
    "\n",
    "            if not os.path.exists(binned_rpm_filename) or recompute_all:      \n",
    "                    info('Make constant binned (%dbp) rpm values file' % bin_size)\n",
    "                    #cmd='bedtools sort -i %s |  bedtools map -a %s -b stdin -c 4 -o mean -null 0.0 | cut -f5 > %s'   %(rpm_filename,genome_sorted_bins_file,binned_rpm_filename)\n",
    "                    #proc=sb.call(cmd,shell=True,env=system_env)\n",
    "                    \n",
    "\n",
    "                    cmd='sort -k1,1 -k2,2n  \"%s\"  > \"%s\"'   %(rpm_filename,sorted_rpm_filename)\n",
    "                    proc=sb.call(cmd,shell=True)\n",
    "\n",
    "                    \n",
    "                    BedTool(rpm_filename).\\\n",
    "                        map(a= sorted_rpm_filename, \n",
    "                            b= genome_sorted_filtered_bins_file, \n",
    "                            c=4, o='mean', null=0.0).\\\n",
    "                        saveas(mapped_sorted_rpm_filename)\n",
    "    \n",
    "                    cmd='cut -f5 \"%s\"  > \"%s\"'   %(mapped_sorted_rpm_filename,binned_rpm_filename)\n",
    "                    proc=sb.call(cmd,shell=True)\n",
    "\n",
    "            \n",
    "            try:    \n",
    "                    os.remove(rpm_filename)\n",
    "                    os.remove(sorted_rpm_filename)\n",
    "                    os.remove(mapped_sorted_rpm_filename)\n",
    "            except:\n",
    "                    pass\n",
    "\n",
    "\n",
    "#load coordinates of bins\n",
    "coordinates_bin=pd.read_csv(genome_sorted_filtered_bins_file,names=['chr_id','bpstart','bpend'],sep='\\t',header=None,usecols=[0,1,2])\n",
    "N_BINS=coordinates_bin.shape[0]\n",
    "if not use_X_Y:\n",
    "    coordinates_bin=coordinates_bin.ix[(coordinates_bin['chr_id']!='chrX') & (coordinates_bin['chr_id']!='chrY')]  \n",
    "\n",
    "\n",
    "\n",
    "genome_windows_filtered\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "coordinates_bin['chr_id'].value_counts()\n",
    "#load all the tracks\n",
    "info('Loading the processed tracks') \n",
    "df_chip={}\n",
    "for state_file in  glob.glob(os.path.join(intermediate_directory,'*.rpm')):\n",
    "        col_name=os.path.basename(state_file).replace('.rpm','')\n",
    "        df_chip[col_name]=pd.read_csv(state_file,squeeze=True,header=None)\n",
    "        info('Loading:%s' % col_name)\n",
    "\n",
    "df_chip=pd.DataFrame(df_chip)\n",
    "\n",
    "if disable_quantile_normalization:\n",
    "        info('Skipping quantile normalization...')\n",
    "else:\n",
    "        info('Normalizing the data...')\n",
    "        df_chip=pd.DataFrame(quantile_normalization(df_chip.values),columns=df_chip.columns,index=df_chip.index)\n",
    "\n",
    "\n",
    "if which('bedGraphToBigWig'): \n",
    "        #write quantile normalized tracks\n",
    "        coord_quantile=coordinates_bin.copy()\n",
    "        for col in df_chip:\n",
    "\n",
    "            if disable_quantile_normalization:\n",
    "                    normalized_output_filename=os.path.join(tracks_directory,'%s.bedgraph' % os.path.basename(col))\n",
    "            else:\n",
    "                    normalized_output_filename=os.path.join(tracks_directory,'%s_quantile_normalized.bedgraph' % os.path.basename(col))\n",
    "                    \n",
    "            normalized_output_filename_bigwig=normalized_output_filename.replace('.bedgraph','.bw')\n",
    "  \n",
    "            if not os.path.exists(normalized_output_filename_bigwig) or recompute_all:         \n",
    "                    info('Writing binned track: %s' % normalized_output_filename_bigwig )    \n",
    "                    coord_quantile['rpm_normalized']=df_chip.ix[:,col]\n",
    "                    coord_quantile.dropna().to_csv(normalized_output_filename,sep='\\t',header=False,index=False)\n",
    "            \n",
    "                    cmd='bedGraphToBigWig %s %s %s' %(normalized_output_filename,chr_len_filename,normalized_output_filename_bigwig)\n",
    "                    proc=sb.call(cmd,shell=True,env=system_env)\n",
    "                    try:\n",
    "                            os.remove(normalized_output_filename)\n",
    "                    except:\n",
    "                            pass\n",
    "else:\n",
    "        info('Sorry I cannot creat the bigwig file.\\nPlease download and install bedGraphToBigWig from here: http://hgdownload.cse.ucsc.edu/admin/exe/ and add to your PATH')\n",
    "     \n",
    "        \n",
    "#th_rpm=np.min(df_chip.apply(lambda x: np.percentile(x,th_rpm)))\n",
    "th_rpm=find_th_rpm(df_chip,th_rpm)\n",
    "info('Estimated th_rpm:%s' % th_rpm)\n",
    "\n",
    "df_chip_not_empty=df_chip.ix[(df_chip>th_rpm).any(1),:]\n",
    "\n",
    "\n",
    "\n",
    "if transformation=='log2':\n",
    "        df_chip_not_empty=df_chip_not_empty.applymap(log2_transform)\n",
    "        info('Using log2 transformation')\n",
    "\n",
    "elif transformation =='angle':     \n",
    "        df_chip_not_empty=df_chip_not_empty.applymap(angle_transform )\n",
    "        info('Using angle transformation')\n",
    "\n",
    "else:\n",
    "        info('Using no transformation')\n",
    "        \n",
    "iod_values=df_chip_not_empty.var(1)/df_chip_not_empty.mean(1)\n",
    "\n",
    "####calculate the inflation point a la superenhancers\n",
    "scores=iod_values\n",
    "min_s=np.min(scores)\n",
    "max_s=np.max(scores)\n",
    "\n",
    "N_POINTS=len(scores)\n",
    "x=np.linspace(0,1,N_POINTS)\n",
    "y=sorted((scores-min_s)/(max_s-min_s))\n",
    "m=smooth((np.diff(y)/np.diff(x)),50)\n",
    "m=m-1\n",
    "m[m<=0]=np.inf\n",
    "m[:int(len(m)*(1-max_regions_percentage))]=np.inf\n",
    "idx_th=np.argmin(m)+1\n",
    "\n",
    "#print idx_th,\n",
    "th_iod=sorted(iod_values)[idx_th]\n",
    "#print th_iod\n",
    "\n",
    "\n",
    "hpr_idxs=iod_values>th_iod\n",
    "#print len(iod_values),len(hpr_idxs),sum(hpr_idxs), sum(hpr_idxs)/float(len(hpr_idxs)),\n",
    "\n",
    "info('Selected %f%% regions (%d)' %( sum(hpr_idxs)/float(len(hpr_idxs))*100, sum(hpr_idxs)))\n",
    "coordinates_bin['iod']=iod_values\n",
    "\n",
    "#we remove the regions \"without\" signal in any of the cell types\n",
    "coordinates_bin.dropna(inplace=True)\n",
    "\n",
    "\n",
    "#create a track for IGV\n",
    "bedgraph_iod_track_filename=os.path.join(tracks_directory,'VARIABILITY.bedgraph')\n",
    "bw_iod_track_filename=os.path.join(tracks_directory,'VARIABILITY.bw')\n",
    "\n",
    "if not os.path.exists(bw_iod_track_filename) or recompute_all:   \n",
    "\n",
    "        info('Generating variability track in bigwig format in:%s' % bw_iod_track_filename)\n",
    "\n",
    "        coordinates_bin.to_csv(bedgraph_iod_track_filename,sep='\\t',header=False,index=False)\n",
    "        sb.call('bedGraphToBigWig %s %s %s' % (bedgraph_iod_track_filename,chr_len_filename,bw_iod_track_filename ),shell=True,env=system_env)\n",
    "        try:\n",
    "                os.remove(bedgraph_iod_track_filename)\n",
    "        except:\n",
    "                pass\n",
    "\n",
    "\n",
    "#Write the HPRs\n",
    "bedgraph_hpr_filename=os.path.join(tracks_directory,'SELECTED_VARIABILITY_HOTSPOT.bedgraph')\n",
    "\n",
    "to_write=coordinates_bin.ix[hpr_idxs[hpr_idxs].index]\n",
    "to_write.dropna(inplace=True)\n",
    "to_write['bpstart']=to_write['bpstart'].astype(int)\n",
    "to_write['bpend']=to_write['bpend'].astype(int)\n",
    "\n",
    "to_write.to_csv(bedgraph_hpr_filename,sep='\\t',header=False,index=False)\n",
    "\n",
    "bed_hpr_fileaname=os.path.join(output_directory,'SELECTED_VARIABILITY_HOTSPOT.bed')\n",
    "\n",
    "if not os.path.exists(bed_hpr_fileaname) or recompute_all:  \n",
    "        info('Writing the HPRs in: %s' % bed_hpr_fileaname)\n",
    "        sb.call('sort -k1,1 -k2,2n %s | bedtools merge -i stdin >  %s' %(bedgraph_hpr_filename,bed_hpr_fileaname),shell=True,env=system_env)\n",
    "\n",
    "#os.remove(bedgraph_hpr_filename)\n",
    "\n",
    "df_chip_hpr=df_chip_not_empty.ix[hpr_idxs,:]\n",
    "df_chip_hpr_zscore=df_chip_hpr.apply(zscore,axis=1)\n",
    "\n",
    "\n",
    "specific_regions_directory=os.path.join(output_directory,'SPECIFIC_REGIONS')\n",
    "if not os.path.exists(specific_regions_directory):\n",
    "        os.makedirs(specific_regions_directory)   \n",
    "\n",
    "if depleted:\n",
    "        z_score_high=-z_score_high\n",
    "        z_score_low=-z_score_low\n",
    "\n",
    "\n",
    "#write target\n",
    "info('Writing Specific Regions for each cell line...')\n",
    "coord_zscore=coordinates_bin.copy()\n",
    "for col in df_chip_hpr_zscore:\n",
    "\n",
    "        regions_specific_filename='Regions_specific_for_%s_z_%.2f.bedgraph' % (os.path.basename(col).replace('.rpm',''),z_score_high)\n",
    "        specific_output_filename=os.path.join(specific_regions_directory,regions_specific_filename)\n",
    "        specific_output_bed_filename=specific_output_filename.replace('.bedgraph','.bed')\n",
    "\n",
    "        if not os.path.exists(specific_output_bed_filename) or recompute_all:  \n",
    "                if depleted:\n",
    "                        coord_zscore['z-score']=df_chip_hpr_zscore.ix[df_chip_hpr_zscore.ix[:,col]<z_score_high,col]\n",
    "                else:\n",
    "                        coord_zscore['z-score']=df_chip_hpr_zscore.ix[df_chip_hpr_zscore.ix[:,col]>z_score_high,col]\n",
    "                coord_zscore.dropna().to_csv(specific_output_filename,sep='\\t',header=False,index=False)\n",
    "\n",
    "                info('Writing:%s' % specific_output_bed_filename )\n",
    "                sb.call('sort -k1,1 -k2,2n %s | bedtools merge -i stdin >  %s' %(specific_output_filename,specific_output_bed_filename),shell=True,env=system_env)\n",
    "\n",
    "\n",
    "#write background\n",
    "info('Writing Background Regions for each cell line...')\n",
    "coord_zscore=coordinates_bin.copy()\n",
    "for col in df_chip_hpr_zscore:\n",
    "\n",
    "        regions_bg_filename='Background_for_%s_z_%.2f.bedgraph' % (os.path.basename(col).replace('.rpm',''),z_score_low)\n",
    "        bg_output_filename=os.path.join(specific_regions_directory,'Background_for_%s_z_%.2f.bedgraph' % (os.path.basename(col).replace('.rpm',''),z_score_low))\n",
    "        bg_output_bed_filename=bg_output_filename.replace('.bedgraph','.bed')\n",
    "\n",
    "        if not os.path.exists(bg_output_bed_filename) or recompute_all:\n",
    "\n",
    "                if depleted:\n",
    "                        coord_zscore['z-score']=df_chip_hpr_zscore.ix[df_chip_hpr_zscore.ix[:,col]>z_score_low,col]\n",
    "                else:\n",
    "                        coord_zscore['z-score']=df_chip_hpr_zscore.ix[df_chip_hpr_zscore.ix[:,col]<z_score_low,col]\n",
    "                coord_zscore.dropna().to_csv(bg_output_filename,sep='\\t',header=False,index=False)\n",
    "\n",
    "                info('Writing:%s' % bg_output_bed_filename )\n",
    "                sb.call('sort -k1,1 -k2,2n -i %s | bedtools merge -i stdin >  %s' %(bg_output_filename,bg_output_bed_filename),shell=True,env=system_env)    \n",
    "\n",
    "\n",
    "###plot selection\n",
    "pl.figure()\n",
    "pl.title('Selection of the HPRs')\n",
    "pl.plot(x,y,'r',lw=3)\n",
    "pl.plot(x[idx_th],y[idx_th],'*',markersize=20)\n",
    "pl.hold(True)\n",
    "x_ext=np.linspace(-0.1,1.2,N_POINTS)\n",
    "y_line=(m[idx_th]+1.0)*(x_ext -x[idx_th])+ y[idx_th];\n",
    "pl.plot(x_ext,y_line,'--k',lw=3)\n",
    "pl.xlim(0,1.1)\n",
    "pl.ylim(0,1)\n",
    "pl.xlabel('Fraction of bins')\n",
    "pl.ylabel('Score normalized')\n",
    "pl.savefig(os.path.join(output_directory,'SELECTION_OF_VARIABILITY_HOTSPOT.pdf'))\n",
    "pl.close()\n",
    "\n",
    "\n",
    "\n",
    "igv_session_filename=os.path.join(output_directory,'OPEN_ME_WITH_IGV.xml')\n",
    "info('Creating an IGV session file (.xml) in: %s' %igv_session_filename)\n",
    "\n",
    "session = ET.Element(\"Session\")\n",
    "session.set(\"genome\",genome_name)\n",
    "session.set(\"hasGeneTrack\",\"true\")\n",
    "session.set(\"version\",\"7\")\n",
    "resources = ET.SubElement(session, \"Resources\")\n",
    "panel= ET.SubElement(session, \"Panel\")\n",
    "\n",
    "resource_items=[]\n",
    "track_items=[]\n",
    "\n",
    "hpr_iod_scores=scores[scores>th_iod]\n",
    "min_h=np.mean(hpr_iod_scores)-2*np.std(hpr_iod_scores)\n",
    "max_h=np.mean(hpr_iod_scores)+2*np.std(hpr_iod_scores)\n",
    "mid_h=np.mean(hpr_iod_scores)\n",
    "#write the tracks\n",
    "for sample_name in sample_names:\n",
    "    if disable_quantile_normalization:\n",
    "            track_full_path=os.path.join(output_directory,'TRACKS','%s.%dbp.bw' % (sample_name,bin_size))\n",
    "    else:\n",
    "            track_full_path=os.path.join(output_directory,'TRACKS','%s.%dbp_quantile_normalized.bw' % (sample_name,bin_size))\n",
    "\n",
    "    track_filename=rem_base_path(track_full_path,output_directory)        \n",
    "\n",
    "    if os.path.exists(track_full_path):    \n",
    "            resource_items.append( ET.SubElement(resources, \"Resource\"))\n",
    "            resource_items[-1].set(\"path\",track_filename)\n",
    "            track_items.append(ET.SubElement(panel, \"Track\" ))\n",
    "            track_items[-1].set('color',\"0,0,178\")\n",
    "            track_items[-1].set('id',track_filename)\n",
    "            track_items[-1].set(\"name\",sample_name)\n",
    "\n",
    "resource_items.append(ET.SubElement(resources, \"Resource\"))\n",
    "resource_items[-1].set(\"path\",rem_base_path(bw_iod_track_filename,output_directory))\n",
    "\n",
    "track_items.append(ET.SubElement(panel, \"Track\" ))\n",
    "track_items[-1].set('color',\"178,0,0\")\n",
    "track_items[-1].set('id',rem_base_path(bw_iod_track_filename,output_directory))\n",
    "track_items[-1].set('renderer',\"HEATMAP\")\n",
    "track_items[-1].set(\"colorScale\",\"ContinuousColorScale;%e;%e;%e;%e;0,153,255;255,255,51;204,0,0\" % (mid_h,min_h,mid_h,max_h))\n",
    "track_items[-1].set(\"name\",'VARIABILITY')\n",
    "\n",
    "resource_items.append(ET.SubElement(resources, \"Resource\"))\n",
    "resource_items[-1].set(\"path\",rem_base_path(bed_hpr_fileaname,output_directory))\n",
    "track_items.append(ET.SubElement(panel, \"Track\" ))\n",
    "track_items[-1].set('color',\"178,0,0\")\n",
    "track_items[-1].set('id',rem_base_path(bed_hpr_fileaname,output_directory))\n",
    "track_items[-1].set('renderer',\"HEATMAP\")\n",
    "track_items[-1].set(\"colorScale\",\"ContinuousColorScale;%e;%e;%e;%e;0,153,255;255,255,51;204,0,0\" % (mid_h,min_h,mid_h,max_h))\n",
    "track_items[-1].set(\"name\",'HOTSPOTS')\n",
    "\n",
    "for sample_name in sample_names:\n",
    "    track_full_path=glob.glob(os.path.join(output_directory,'SPECIFIC_REGIONS','Regions_specific_for_%s*.bedgraph' %sample_name))[0]    \n",
    "    specific_track_filename=rem_base_path(track_full_path,output_directory)\n",
    "    if os.path.exists(track_full_path):\n",
    "            resource_items.append( ET.SubElement(resources, \"Resource\"))\n",
    "            resource_items[-1].set(\"path\",specific_track_filename)\n",
    "\n",
    "            track_items.append(ET.SubElement(panel, \"Track\" ))\n",
    "            track_items[-1].set('color',\"178,0,0\")\n",
    "            track_items[-1].set('id',specific_track_filename)\n",
    "            track_items[-1].set('renderer',\"HEATMAP\")\n",
    "            track_items[-1].set(\"colorScale\",\"ContinuousColorScale;%e;%e;%e;%e;0,153,255;255,255,51;204,0,0\" % (mid_h,min_h,mid_h,max_h))\n",
    "            track_items[-1].set(\"name\",'REGION SPECIFIC FOR %s' % sample_name)\n",
    "\n",
    "tree = ET.ElementTree(session)\n",
    "tree.write(igv_session_filename,xml_declaration=True)\n",
    "\n",
    "info('All done! Ciao!')\n",
    "sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_names_hotspots_filename' is not defined",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-7ba692beee74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#CALL HAYSTACK HOTSPOTS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m cmd_to_run='haystack_hotspots \"%s\" %s --output_directory \"%s\" --bin_size %d %s %s %s %s %s %s %s %s' %             (sample_names_hotspots_filename, genome_name,output_directory,bin_size,\n\u001b[0m\u001b[1;32m      3\u001b[0m              \u001b[0;34m(\u001b[0m\u001b[0;34m'--recompute_all'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrecompute_all\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m              \u001b[0;34m(\u001b[0m\u001b[0;34m'--depleted'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdepleted\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m              \u001b[0;34m(\u001b[0m\u001b[0;34m'--input_is_bigwig'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minput_is_bigwig\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sample_names_hotspots_filename' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "#CALL HAYSTACK MOTIFS\n",
    "motif_directory=os.path.join(output_directory,'HAYSTACK_MOTIFS')\n",
    "for sample_name in sample_names:\n",
    "    specific_regions_filename=os.path.join(output_directory,'HAYSTACK_HOTSPOTS','SPECIFIC_REGIONS','Regions_specific_for_%s*.bed' %sample_name)\n",
    "    bg_regions_filename=glob.glob(os.path.join(output_directory,'HAYSTACK_HOTSPOTS','SPECIFIC_REGIONS','Background_for_%s*.bed' %sample_name))[0]\n",
    "    #bg_regions_filename=glob.glob(specific_regions_filename.replace('Regions_specific','Background')[:-11]+'*.bed')[0] #lo zscore e' diverso...\n",
    "    #print specific_regions_filename,bg_regions_filename\n",
    "    cmd_to_run='haystack_motifs %s %s --bed_bg_filename %s --output_directory %s --name %s' % (specific_regions_filename,genome_name, bg_regions_filename,motif_directory, sample_name)\n",
    "    \n",
    "    if meme_motifs_filename:\n",
    "         cmd_to_run+=' --meme_motifs_filename %s' % meme_motifs_filename\n",
    "         \n",
    "         \n",
    "    if n_processes:\n",
    "        cmd_to_run+=' --n_processes %d' % n_processes\n",
    "        \n",
    "    if temp_directory:\n",
    "        cmd_to_run+=' --temp_directory %s' % temp_directory\n",
    "        \n",
    "        \n",
    "    \n",
    "    print cmd_to_run\n",
    "    sb.call(cmd_to_run,shell=True,env=system_env)\n",
    "\n",
    "    if USE_GENE_EXPRESSION:\n",
    "            #CALL HAYSTACK TF ACTIVITY \n",
    "            motifs_output_folder=os.path.join(motif_directory,'HAYSTACK_MOTIFS_on_%s' % sample_name) \n",
    "            if os.path.exists(motifs_output_folder):\n",
    "                cmd_to_run='haystack_tf_activity_plane %s %s %s --output_directory %s'  %(motifs_output_folder,sample_names_tf_activity_filename,sample_name,tf_activity_directory)\n",
    "                \n",
    "                if motif_mapping_filename:\n",
    "                    cmd_to_run+=' --motif_mapping_filename %s' %  motif_mapping_filename       \n",
    "                \n",
    "                if plot_all:\n",
    "                    cmd_to_run+=' --plot_all'\n",
    "                    \n",
    "                \n",
    "                print cmd_to_run\n",
    "                sb.call(cmd_to_run,shell=True,env=system_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    \n",
    "                    outfile = pysam.AlignmentFile(bam_filename+\"filt.sam\", \"w\", template=infile)\n",
    "                       for s in infile.fetch(until_eof=True):\n",
    "                         if not s.is_qcfail:\n",
    "                         outfile.write(s)\n",
    "                     outfile.close()\n",
    "                    \n",
    "                    infile.close()\n",
    "                    \n",
    "                    outfile_path= 'cd K562H3k27ac_sorted_rmdup.filt.sam'\n",
    "\n",
    "                    rows = pysam.view(\"-b\", \"-F\", \"1804\", \"-o\", \"%s\" %outfile_path, bam_filename)\n",
    "                    for r in rows:\n",
    "                        print r\n",
    "                \n",
    "                   \n",
    "\t\n",
    "                samtools view -F 1804 -q $mapq_thresh -u $bam | sambamba sort -t $nth /dev/stdin -o $filt_bam \n",
    "    \n",
    "                #not primary alignment\n",
    "                #read fails platform/vendor quality checks\n",
    "                #read is PCR or \n",
    "      \n",
    "            \n",
    "\n",
    "            # try:    \n",
    "            #     os.remove(bam_filtered_nodup_filename)\n",
    "            #     os.remove(bam_filtered_nodup_filename+'.bai')\n",
    "            # \n",
    "            # except:\n",
    "            #         pass\n",
    "            # \n",
    "            # cmd= 'sambamba view  -f bam -l 0 -t %d -F  \"not (unmapped or mate_is_unmapped or failed_quality_control or duplicate or secondary_alignment) and proper_pair\" \"%s\"  -o \"%s\"' %(n_processes, bam_temp_filename, bam_filtered_nodup_filename )\n",
    "            # proc=sb.call(cmd,shell=True)\n",
    "\n",
    "\n",
    "\t\n",
    "\n",
    "\n",
    "                    cmd='samtools view -F 1804 -f 2 -u \"-o\" \"%s\" %'outfile_path, bam_filename' \n",
    "                    #cmd='samtools view -F 1804 -f 2 -u \"-o\" \"%s\" %outfile_path, bam_filename | bamToBed | slopBed  -r %s -l 0 -s -i stdin -g %s | genomeCoverageBed -g  %s -i stdin -bg -scale %.32f > %s' \n",
    "                #  %(bam_filename,bin_size,chr_len_filename,chr_len_filename,scaling_factor,rpm_filename)\n",
    "                    #print cmd\n",
    "        \n",
    "                    proc=sb.call(cmd,shell=True)\n",
    "\n",
    "                \n",
    "\n",
    "                    info('Scaling Factor: %e' %scaling_factor)\n",
    "\n",
    "                    \n",
    "                   BedTool(bam_filtered_nodup_filename).bam_to_bed().\\\n",
    "                        slop(r=bin_size,l=0, s=True, g=chr_len_filtered_filename).saveas(bam_filtered_nodup_filename+'.slop.bed')\\\n",
    "                        genome_coverage(genome=genome_name, bg=True,scale= '{:.32f}'.format(scaling_factor)).\\\n",
    "                        saveas(rpm_filename)\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    # args = ['-b', '\"%s\"'%bam_filtered_nodup_filename,\n",
    "    #         '-o', '\"%s\"'%rpm_filename,\n",
    "    #         '--blackListFileName', '\"%s\"'%bam_filtered_nodup_filename,\n",
    "    #         '--binSize', '%d'%bin_size ,\n",
    "    #         '--normalizeTo1x', '2150570000',\n",
    "    #         '--extendReads', '%d'%bin_size,\n",
    "    #         '--outFileFormat','bedgraph',\n",
    "    #         '--ignoreForNormalization', 'chrX'] %(bam_filtered_nodup_filename, rpm_filename, blacklist, bin_size, bin_size)\n",
    "\n",
    "    bam_cov.main(args)\n",
    "    genome_sorted_filtered_bins_file\n",
    "                    #cmd='samtools view -b -F 512 %s | bamToBed | slopBed  -r %s -l 0 -s -i stdin -g %s | genomeCoverageBed -g  %s -i stdin -bg -scale %.32f > %s'  %(bam_filename,bin_size,chr_len_filename,chr_len_filename,scaling_factor,rpm_filename)\n",
    "                    #print cmd \n",
    "            \n",
    "                    proc=sb.call(cmd,shell=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}